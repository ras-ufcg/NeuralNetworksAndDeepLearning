# The four fundamental equations behind backpropagation

Backpropagation is about understanding how changing the weights and biases in a network changes the cost function. Ultimately, this means computing the partial derivatives 
<img src="https://latex.codecogs.com/png.latex?%5Csmall%20%5Cpartial%20C/%5Cpartial%20w%5E%7Bl%7D_%7Bjk%7D"/>  and  <img src="https://latex.codecogs.com/png.latex?%5Csmall%20%5Cpartial%20C/%5Cpartial%20b%5E%7Bl%7D_%7Bj%7D"/>.  But to compute those, we first introduce an intermediate quantity, <img src="https://latex.codecogs.com/png.latex?%5Csmall%20%5Cdelta_%7Bj%7D%5E%7Bl%7D"/> which we call the *error* in the <img src="https://latex.codecogs.com/png.latex?%5Csmall%20j%5E%7Bth%7D"/> neuron in the <img src="https://latex.codecogs.com/png.latex?%5Csmall%20l%5E%7Bth%7D"/> layer.
Backpropagation will give us a procedure to compute the error <img src="https://latex.codecogs.com/png.latex?%5Csmall%20%5Cdelta_%7Bj%7D%5E%7Bl%7D"/>, and then will relate <img src="https://latex.codecogs.com/png.latex?%5Csmall%20%5Cdelta_%7Bj%7D%5E%7Bl%7D"/> to <img src="https://latex.codecogs.com/png.latex?%5Csmall%20%5Cpartial%20C/%5Cpartial%20w%5E%7Bl%7D_%7Bjk%7D"/>  and  <img src="https://latex.codecogs.com/png.latex?%5Csmall%20%5Cpartial%20C/%5Cpartial%20b%5E%7Bl%7D_%7Bj%7D"/>.

To understand how the error is defined, imagine there is a demon in our neural network:

<img align="center" src="http://neuralnetworksanddeeplearning.com/images/tikz19.png"/>

The demon sits at the <img src="https://latex.codecogs.com/png.latex?%5Csmall%20%5Cdelta_%7Bj%7D%5E%7Bl%7D"/> neuron in layer l. As the input to the neuron comes in, the demon messes with the neuron's operation. It adds a little change <img src="https://latex.codecogs.com/png.latex?%5Csmall%20%5CDelta%20z%5E%7Bl%7D_%7Bj%7D"/> to the neuron's weighted input, so that instead of outputting <img src="https://latex.codecogs.com/png.latex?%5Csmall%20%5Csigma%20%28z%5E%7Bl%7D_%7Bj%7D%29"/>, the neuron instead outputs <img src="https://latex.codecogs.com/png.latex?%5Csmall%20%5Csigma%20%28z%5E%7Bl%7D_%7Bj%7D%20&plus;%20%5CDelta%20z%5E%7Bl%7D_%7Bj%7D%29"/>. This change propagates through later layers in the network, finally causing the overall cost to change by an amount <img src="https://latex.codecogs.com/png.latex?%5Csmall%20%5Cfrac%7B%5Cpartial%20C%7D%7B%5Cpartial%20z%5E%7Bl%7D_%7Bj%7D%7D%5CDelta%20z%5E%7Bl%7D_%7Bj%7D"/>.

Now, this demon is a good demon, and is trying to help you improve the cost, i.e., they're trying to find a <img src="https://latex.codecogs.com/png.latex?%5Csmall%20%5CDelta%20z%5E%7Bl%7D_%7Bj%7D"/> which makes the cost smaller. Suppose <img src="https://latex.codecogs.com/png.latex?%5Csmall%20%5Cfrac%7B%5Cpartial%20C%7D%7B%5Cpartial%20z%5E%7Bl%7D_%7Bj%7D%7D"/> has a large value (either positive or negative). Then the demon can lower the cost quite a bit by choosing <img src="https://latex.codecogs.com/png.latex?%5Csmall%20%5CDelta%20z%5E%7Bl%7D_%7Bj%7D"/> to have the opposite sign to <img src="https://latex.codecogs.com/png.latex?%5Csmall%20%5Cfrac%7B%5Cpartial%20C%7D%7B%5Cpartial%20z%5E%7Bl%7D_%7Bj%7D%7D"/>.
By contrast, if <img src="https://latex.codecogs.com/png.latex?%5Csmall%20%5Cfrac%7B%5Cpartial%20C%7D%7B%5Cpartial%20z%5E%7Bl%7D_%7Bj%7D%7D"/> is close to zero, then the demon can't improve the cost much at all by perturbing the weighted input <img src="https://latex.codecogs.com/png.latex?%5Csmall%20z%5E%7Bl%7D_%7Bj%7D"/>. So far as the demon can tell, the neuron is already pretty near optimal\*.  And so there's a heuristic sense in which <img src="https://latex.codecogs.com/png.latex?%5Csmall%20%5Cfrac%7B%5Cpartial%20C%7D%7B%5Cpartial%20z%5E%7Bl%7D_%7Bj%7D%7D"/> is a measure of the error in the neuron.

> \* This is only the case for small changes <img src="https://latex.codecogs.com/png.latex?%5Csmall%20%5CDelta%20z%5E%7Bl%7D_%7Bj%7D"/>, of course. We'll assume that the demon is constrained to make such small changes.

Motivated by this story, we define the error <img src="https://latex.codecogs.com/png.latex?%5Csmall%20%5Cdelta_%7Bj%7D%5E%7Bl%7D"/> of neuron j in layer l by

<img align="center" src="https://latex.codecogs.com/png.latex?%5Cdelta_%7Bj%7D%5E%7Bl%7D%20%5Cequiv%20%5Cfrac%7B%5Cpartial%20C%7D%7B%5Cpartial%20z_%7Bj%7D%5E%7Bl%7D%7D"/> (29)

As per our usual conventions, we use <img src="https://latex.codecogs.com/png.latex?%5Csmall%20%5Cdelta^l"/> to denote the vector of errors associated with layer l. Backpropagation will give us a way of computing <img src="https://latex.codecogs.com/png.latex?%5Csmall%20%5Cdelta_l"/> for every layer, and then relating those errors to the quantities of real interest, <img src="https://latex.codecogs.com/png.latex?%5Csmall%20%5Cpartial%20C/%5Cpartial%20w%5E%7Bl%7D_%7Bjk%7D"/> and <img src="https://latex.codecogs.com/png.latex?%5Csmall%20%5Cpartial%20C/%5Cpartial%20b%5E%7Bl%7D_%7Bj%7D"/>.

You might wonder why the demon is changing the weighted input <img src="https://latex.codecogs.com/png.latex?%5Csmall%20z%5E%7Bl%7D_%7Bj%7D"/>. Surely it'd be more natural to imagine the demon changing the output activation <img src="https://latex.codecogs.com/png.latex?%5Csmall%20a_%7Bj%7D%5E%7Bl%7D"/>, with the result that we'd be using <img src="https://latex.codecogs.com/png.latex?%5Csmall%20%5Cfrac%7B%5Cpartial%20C%7D%7B%5Cpartial%20a_%7Bj%7D%5E%7Bl%7D%7D"/> as our measure of error. In fact, if you do this things work out quite similarly to the discussion below. But it turns out to make the presentation of backpropagation a little more algebraically complicated. So we'll stick with <img src="https://latex.codecogs.com/png.latex?%5Csmall%20%5Cdelta_%7Bj%7D%5E%7Bl%7D%20%3D%20%5Cfrac%7B%5Cpartial%20C%7D%7B%5Cpartial%20z_%7Bj%7D%5E%7Bl%7D%7D"/> as our measure of error.\*

> \* In classification problems like MNIST the term "error" is sometimes used to mean the classification failure rate. E.g., if the neural net correctly classifies 96.0 percent of the digits, then the error is 4.0 percent. Obviously, this has quite a different meaning from our δ vectors. In practice, you shouldn't have trouble telling which meaning is intended in any given usage.

**Plan of attack:** Backpropagation is based around four fundamental equations. Together, those equations give us a way of computing both the error <img src="https://latex.codecogs.com/png.latex?%5Csmall%20%5Cdelta^l"/> and the gradient of the cost function. I state the four equations below. Be warned, though: you shouldn't expect to instantaneously assimilate the equations. Such an expectation will lead to disappointment. In fact, the backpropagation equations are so rich that understanding them well requires considerable time and patience as you gradually delve deeper into the equations. The good news is that such patience is repaid many times over. And so the discussion in this section is merely a beginning, helping you on the way to a thorough understanding of the equations.

Here's a preview of the ways we'll delve more deeply into the equations later in the chapter: [I'll give a short proof of the equations](https://github.com/ras-ufcg/NeuralNetworksAndDeepLearning/blob/main/2/2_5.md), which helps explain why they are true; we'll [restate the equations](https://github.com/ras-ufcg/NeuralNetworksAndDeepLearning/blob/main/2/2_6.md) in algorithmic form as pseudocode, and [see how](https://github.com/ras-ufcg/NeuralNetworksAndDeepLearning/blob/main/2/2_7.md) the pseudocode can be implemented as real, running Python code; and, in the [final section of the chapter](https://github.com/ras-ufcg/NeuralNetworksAndDeepLearning/blob/main/2/2_9.md), we'll develop an intuitive picture of what the backpropagation equations mean, and how someone might discover them from scratch. Along the way we'll return repeatedly to the four fundamental equations, and as you deepen your understanding those equations will come to seem comfortable and, perhaps, even beautiful and natural.

**An equation for the error in the output layer,** <img src="https://latex.codecogs.com/png.latex?%5Csmall%20%5Cdelta^L"/>: The components of <img src="https://latex.codecogs.com/png.latex?%5Csmall%20%5Cdelta^L"/> are given by

<img align="center" src="https://latex.codecogs.com/png.latex?%5Csmall%20%5Cdelta_%7Bj%7D%5E%7BL%7D%20%3D%20%5Cfrac%7B%5Cpartial%20C%7D%7B%5Cpartial%20a_%7Bj%7D%5E%7BL%7D%7D%5Csigma%27%28z_%7Bj%7D%5E%7BL%7D%29"/> (BP1)

This is a very natural expression. The first term on the right, <img src="https://latex.codecogs.com/png.latex?%5Csmall%20%5Cpartial%20C/%5Cpartial%20a_%7Bj%7D%5E%7BL%7D"/>, just measures how fast the cost is changing as a function of the jth output activation. If, for example, C doesn't depend much on a particular output neuron, j, then <img src="https://latex.codecogs.com/png.latex?%5Csmall%20%5Cdelta_%7Bj%7D%5E%7BL%7D"/> will be small, which is what we'd expect. The second term on the right, <img src="https://latex.codecogs.com/png.latex?%5Csmall%20%5Csigma%27%28z_%7Bj%7D%5E%7BL%7D%29"/>, measures how fast the activation function <img src="https://latex.codecogs.com/png.latex?%5Csmall%20%5Csigma"/> is changing at <img src="https://latex.codecogs.com/png.latex?%5Csmall%20z_%7Bj%7D%5E%7BL%7D"/>.

Notice that everything in (BP1) is easily computed. In particular, we compute <img src="https://latex.codecogs.com/png.latex?%5Csmall%20z_%7Bj%7D%5E%7BL%7D"/>. while computing the behaviour of the network, and it's only a small additional overhead to compute <img src="https://latex.codecogs.com/png.latex?%5Csmall%20%5Csigma%27%28z_%7Bj%7D%5E%7BL%7D%29"/>. The exact form of <img src="https://latex.codecogs.com/png.latex?%5Csmall%20%5Cpartial%20C/%5Cpartial%20a_%7Bj%7D%5E%7BL%7D"/> will, of course, depend on the form of the cost function. However, provided the cost function is known there should be little trouble computing <img src="https://latex.codecogs.com/png.latex?%5Csmall%20%5Cpartial%20C/%5Cpartial%20a_%7Bj%7D%5E%7BL%7D"/>. For example, if we're using the quadratic cost function then <img src="https://latex.codecogs.com/png.latex?%5Csmall%20C%20%3D%20%5Cfrac%7B1%7D%7B2%7D%5Csum_%7Bj%7D%28y_j%20-%20a_j%5EL%29%5E2"/>, and so <img src="https://latex.codecogs.com/png.latex?%5Csmall%20%5Cpartial%20C%20/%20%5Cpartial%20a_j%5EL%20%3D%20%28a_j%5EL%20-%20y_j%29"/>, which obviously is easily computable.

Equation (BP1) is a componentwise expression for <img src="https://latex.codecogs.com/png.latex?%5Csmall%20%5Cdelta%5E%7BL%7D"/>. It's a perfectly good expression, but not the matrix-based form we want for backpropagation. However, it's easy to rewrite the equation in a matrix-based form, as

<img align="center" src="https://latex.codecogs.com/png.latex?%5Csmall%20%5Cdelta%5E%7BL%7D%20%3D%20%5Cbigtriangledown_a%20C%5Codot%20%5Csigma%27%28z%5EL%29"/> (BP1a)

Here, <img src="https://latex.codecogs.com/png.latex?%5Cbigtriangledown_a%20C"/> is defined to be a vector whose components are the partial derivatives <img src="https://latex.codecogs.com/png.latex?%5Csmall%20%5Cpartial%20C/%5Cpartial%20a_%7Bj%7D%5E%7BL%7D"/>. You can think of <img src="https://latex.codecogs.com/png.latex?%5Cbigtriangledown_a%20C"/> as expressing the rate of change of C with respect to the output activations. It's easy to see that Equations (BP1a) and (BP1) are equivalent, and for that reason from now on we'll use (BP1) interchangeably to refer to both equations. As an example, in the case of the quadratic cost we have <img src="https://latex.codecogs.com/png.latex?%5Csmall%20%5Cbigtriangledown%20_a%20C%20%3D%20%28a%5EL%20-%20y%29"/>, and so the fully matrix-based form of (BP1) becomes

<img align="center" src="https://latex.codecogs.com/png.latex?%5Csmall%20%5Cdelta%5EL%20%3D%20%28a%5EL%20-%20y%29%5Codot%5Csigma%27%28z%5EL%29"/> (30)

As you can see, everything in this expression has a nice vector form, and is easily computed using a library such as Numpy.

**An equation for <img src="https://latex.codecogs.com/png.latex?%5Csmall%20%5Cdelta%5El"/> in terms of the error in the next layer, <img src="https://latex.codecogs.com/png.latex?%5Csmall%20%5Cdelta%5E%7Bl&plus;1%7D"/>:** In particular

<img align="center" src="https://latex.codecogs.com/png.latex?%5Csmall%20%5Cdelta%5E%7Bl%7D%20%3D%20%28%28w%5E%7Bl&plus;1%7D%29%5ET%5Cdelta%5E%7Bl&plus;1%7D%29%5Codot%20%5Csigma%27%28z%5El%29"/> (BP2)

where <img src="https://latex.codecogs.com/png.latex?%5Csmall%20%28w%5E%7Bl&plus;1%7D%29%5ET"/> is the transpose of the weight matrix <img src="https://latex.codecogs.com/png.latex?%5Csmall%20w%5E%7Bl&plus;1%7D"/> for the (l+1)th layer. This equation appears complicated, but each element has a nice interpretation. Suppose we know the error <img src="https://latex.codecogs.com/png.latex?%5Csmall%20%5Cdelta%5E%7Bl&plus;1%7D"/> at the l+1th layer. When we apply the transpose weight matrix, <img src="https://latex.codecogs.com/png.latex?%5Csmall%20%28w%5E%7Bl&plus;1%7D%29%5ET"/>, we can think intuitively of this as moving the error backward through the network, giving us some sort of measure of the error at the output of the lth layer. We then take the Hadamard product <img src="https://latex.codecogs.com/png.latex?%5Csmall%20%5Codot%20%5Csigma%27%28z%5El%29"/>. This moves the error backward through the activation function in layer l, giving us the error <img src="https://latex.codecogs.com/png.latex?%5Csmall%20%5Cdelta%5El"/> in the weighted input to layer l.

By combining (BP2) with (BP1) we can compute the error <img src="https://latex.codecogs.com/png.latex?%5Csmall%20%5Cdelta%5El"/> for any layer in the network. We start by using (BP1) to compute <img src="https://latex.codecogs.com/png.latex?%5Csmall%20%5Cdelta%5EL"/>, then apply Equation (BP2) to compute <img src="https://latex.codecogs.com/png.latex?%5Csmall%20%5Cdelta%5E%7BL-1%7D"/>, then Equation (BP2) again to compute <img src="https://latex.codecogs.com/png.latex?%5Csmall%20%5Cdelta%5E%7BL-2%7D"/>, and so on, all the way back through the network.

** An equation for the rate of change of the cost with respect to any bias in the network:** In particular:

<img align="center" src="https://latex.codecogs.com/png.latex?%5Csmall%20%5Cfrac%7B%5Cpartial%20C%7D%7B%5Cpartial%20b%5El_j%7D%20%3D%20%5Cdelta%5El_j"/> (BP3)

That is, the error <img src="https://latex.codecogs.com/png.latex?%5Csmall%20%5Cdelta%5El_j"/> is exactly equal to the rate of change <img src="https://latex.codecogs.com/png.latex?%5Csmall%20%5Cpartial%20C%20/%20%5Cpartial%20b%5El_j"/>. This is great news, since (BP1) and (BP2) have already told us how to compute <img src="https://latex.codecogs.com/png.latex?%5Csmall%20%5Cdelta%5El_j"/>. We can rewrite (BP3) in shorthand as

<img align="center" src="https://latex.codecogs.com/png.latex?%5Csmall%20%5Cfrac%7B%5Cpartial%20C%7D%7B%5Cpartial%20b%7D%20%3D%20%5Cdelta%2C"/> (31)

where it is understood that δ is being evaluated at the same neuron as the bias b.

**An equation for the rate of change of the cost with respect to any weight in the network:** In particular:

<img align="center" src="https://latex.codecogs.com/png.latex?%5Csmall%20%5Cfrac%7B%5Cpartial%20C%7D%7B%5Cpartial%20w%5El_%7Bjk%7D%7D%20%3D%20a%5E%7Bl-1%7D_k%20%5Cdelta%5El_j"/> (BP4)

This tells us how to compute the partial derivatives <img src="https://latex.codecogs.com/png.latex?%5Csmall%20%5Cpartial%20C%20/%20%5Cpartial%20w%5El_%7Bjk%7D"/> in terms of the quantities <img src="https://latex.codecogs.com/png.latex?%5Csmall%20%5Cdelta%5El"/> and <img src="https://latex.codecogs.com/png.latex?%5Csmall%20a%5E%7Bl-1%7D"/>, which we already know how to compute. The equation can be rewritten in a less index-heavy notation as

<img align="center" src="https://latex.codecogs.com/png.latex?%5Csmall%20%5Cfrac%7B%5Cpartial%20C%7D%7B%5Cpartial%20w%7D%20%3D%20a_%7B%5Crm%20in%7D%20%5Cdelta_%7B%5Crm%20out%7D%2C"/> (32)

where it's understood that <img src="https://latex.codecogs.com/png.latex?%5Csmall%20a_%7B%5Crm%20in%7D"/> is the activation of the neuron input to the weight w, and <img src="https://latex.codecogs.com/png.latex?%5Csmall%20%5Cdelta_%7B%5Crm%20out%7D"/> is the error of the neuron output from the weight w. Zooming in to look at just the weight w, and the two neurons connected by that weight, we can depict this as:

<img align="center" src="http://neuralnetworksanddeeplearning.com/images/tikz20.png"/>

A nice consequence of Equation (32) is that when the activation <img src="https://latex.codecogs.com/png.latex?%5Csmall%20a_%7B%5Crm%20in%7D"/> is small, <img src="https://latex.codecogs.com/png.latex?%5Csmall%20a_%7B%5Crm%20in%7D%20%5Capprox%200"/>, the gradient term <img src="https://latex.codecogs.com/png.latex?%5Csmall%20%5Cpartial%20C%20/%20%5Cpartial%20w"/> will also tend to be small. In this case, we'll say the weight learns slowly, meaning that it's not changing much during gradient descent. In other words, one consequence of (BP4) is that weights output from low-activation neurons learn slowly.

There are other insights along these lines which can be obtained from (BP1)-(BP4). Let's start by looking at the output layer. Consider the term <img src="https://latex.codecogs.com/png.latex?%5Csmall%20%5Csigma%27%28z%5EL_j%29"/> in (BP1). Recall from the graph of the sigmoid function in the last chapter that the σ function becomes very flat when <img src="https://latex.codecogs.com/png.latex?%5Csmall%20%5Csigma%28z%5EL_j%29"/> is approximately 0 or 1. When this occurs we will have <img src="https://latex.codecogs.com/png.latex?%5Csmall%20%5Csigma%27%28z%5EL_j%29%20%5Capprox%200"/>. And so the lesson is that a weight in the final layer will learn slowly if the output neuron is either low activation (≈ 0) or high activation (≈ 1). In this case it's common to say the output neuron has saturated and, as a result, the weight has stopped learning (or is learning slowly). Similar remarks hold also for the biases of output neuron.

We can obtain similar insights for earlier layers. In particular, note the <img src="https://latex.codecogs.com/png.latex?%5Csmall%20%5Csigma%27%28z%5EL_j%29"/> term in (BP2). This means that <img src="https://latex.codecogs.com/png.latex?%5Csmall%20%5Cdelta%5El_j"/> is likely to get small if the neuron is near saturation. And this, in turn, means that any weights input to a saturated neuron will learn slowly*.

> \* This reasoning won't hold if <img src="https://latex.codecogs.com/png.latex?%5Csmall%20%7Bw%5E%7Bl&plus;1%7D%7D%5ET%20%5Cdelta%5E%7Bl&plus;1%7D"/> has large enough entries to compensate for the smallness of <img src="https://latex.codecogs.com/png.latex?%5Csmall%20%5Csigma%27%28z%5EL_j%29"/>. But I'm speaking of the general tendency.

Summing up, we've learnt that a weight will learn slowly if either the input neuron is low-activation, or if the output neuron has saturated, i.e., is either high- or low-activation.

None of these observations is too greatly surprising. Still, they help improve our mental model of what's going on as a neural network learns. Furthermore, we can turn this type of reasoning around. The four fundamental equations turn out to hold for any activation function, not just the standard sigmoid function (that's because, as we'll see in a moment, the proofs don't use any special properties of σ). And so we can use these equations to design activation functions which have particular desired learning properties. As an example to give you the idea, suppose we were to choose a (non-sigmoid) activation function σ so that σ′ is always positive, and never gets close to zero. That would prevent the slow-down of learning that occurs when ordinary sigmoid neurons saturate. Later in the book we'll see examples where this kind of modification is made to the activation function. Keeping the four equations (BP1)-(BP4) in mind can help explain why such modifications are tried, and what impact they can have.

> Summary: the equations of backpropagation
> 
> <img align="center" src="https://latex.codecogs.com/png.latex?%5Csmall%20%5Cdelta_%7Bj%7D%5E%7BL%7D%20%3D%20%5Cfrac%7B%5Cpartial%20C%7D%7B%5Cpartial%20a_%7Bj%7D%5E%7BL%7D%7D%5Csigma%27%28z_%7Bj%7D%5E%7BL%7D%29"/> (BP1)
>
> <img align="center" src="https://latex.codecogs.com/png.latex?%5Csmall%20%5Cdelta%5E%7Bl%7D%20%3D%20%28%28w%5E%7Bl&plus;1%7D%29%5ET%5Cdelta%5E%7Bl&plus;1%7D%29%5Codot%20%5Csigma%27%28z%5El%29"/> (BP2)
>
> <img align="center" src="https://latex.codecogs.com/png.latex?%5Csmall%20%5Cfrac%7B%5Cpartial%20C%7D%7B%5Cpartial%20b%5El_j%7D%20%3D%20%5Cdelta%5El_j"/> (BP3)
>
> <img align="center" src="https://latex.codecogs.com/png.latex?%5Csmall%20%5Cfrac%7B%5Cpartial%20C%7D%7B%5Cpartial%20w%5El_%7Bjk%7D%7D%20%3D%20a%5E%7Bl-1%7D_k%20%5Cdelta%5El_j"/> (BP4)

## Problem
Alternate presentation of the equations of backpropagation: I've stated the equations of backpropagation (notably (BP1) and (BP2)) using the Hadamard product. This presentation may be disconcerting if you're unused to the Hadamard product. There's an alternative approach, based on conventional matrix multiplication, which some readers may find enlightening. (1) Show that (BP1) may be rewritten as

<img align="center" src="https://latex.codecogs.com/png.latex?%5Csmall%20%5Cdelta%5EL%20%3D%20%5CSigma%27%28z%5EL%29%20%5Cnabla_a%20C%2C"/> (33)
 
where <img src="https://latex.codecogs.com/png.latex?%5Csmall%20%5CSigma%27%28z%5EL%29"/> is a square matrix whose diagonal entries are the values <img src="https://latex.codecogs.com/png.latex?%5Csmall%20%5Csigma%27%28z%5EL_j%29"/>, and whose off-diagonal entries are zero. Note that this matrix acts on <img src="https://latex.codecogs.com/png.latex?%5Csmall%20%5Cnabla_a%20C"/> by conventional matrix multiplication. (2) Show that (BP2) may be rewritten as

<img align="center" src="https://latex.codecogs.com/png.latex?%5Csmall%20%5Cdelta%5El%20%3D%20%5CSigma%27%28z%5El%29%20%28w%5E%7Bl&plus;1%7D%29%5ET%20%5Cdelta%5E%7Bl&plus;1%7D"/> (34)

(3) By combining observations (1) and (2) show that

<img align="center" src="https://latex.codecogs.com/png.latex?%5Csmall%20%5Cdelta%5El%20%3D%20%5CSigma%27%28z%5El%29%20%28w%5E%7Bl&plus;1%7D%29%5ET%20%5Cldots%20%5CSigma%27%28z%5E%7BL-1%7D%29%20%28w%5EL%29%5ET%20%5CSigma%27%28z%5EL%29%20%5Cnabla_a%20C"/> (35)

For readers comfortable with matrix multiplication this equation may be easier to understand than (BP1) and (BP2). The reason I've focused on (BP1) and (BP2) is because that approach turns out to be faster to implement numerically.
